#!/usr/bin/env python3
"""
Logs MCP Server (Loki + JSONL Fallback)

Provides efficient, bounded queries to a Loki backend OR local JSONL files for developer/agent workflows.

Env:
  LOGS_BACKEND: "loki" or "jsonl" (default: auto-detect)
  LOKI_BASE_URL: "http://localhost:3100" (for Loki backend)
  LOKI_TENANT: optional tenant header value for multi-tenant Loki
  LOKI_TOKEN: optional bearer token
  LOGS_JSONL_DIR: path to JSONL log directory (default: "./logs")
  LOGS_DEFAULT_LIMIT: default max entries to return (e.g., "500")
  LOGS_TIMEOUT_MS: HTTP timeout in ms (e.g., "10000")
  LOGS_MAX_BYTES: max response size in bytes (e.g., "200000")

Implements JSON-RPC over stdio with:
  - initialize
  - tools/list
  - tools/call

Tools:
  - logs_query_range(query, start, end, limit?, direction?, labels?)
  - logs_tail(query, since?, limit?, labels?)
  - logs_count(query, start, end, labels?)
  - logs_labels(prefix?)

Safety:
  - Per-call timeout
  - Result truncation by bytes
  - Limit caps to avoid excessive payloads

Backend Auto-detection:
  - Tries Loki first if LOKI_BASE_URL is set
  - Falls back to JSONL files automatically if Loki unreachable
  - JSONL provides same query interface for offline/local operation
"""

import sys
import os
import json
import asyncio
import logging
import gzip
import glob
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime, timedelta, timezone

import httpx

# Basic logging (stderr)
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("logs-mcp")

# Env helpers
def _get_env_str(name: str, default: str = "") -> str:
    v = os.getenv(name)
    return v if v is not None else default

def _get_env_int(name: str, default: int) -> int:
    v = os.getenv(name)
    try:
        return int(v) if v is not None else default
    except Exception:
        return default

LOGS_BACKEND: str = _get_env_str("LOGS_BACKEND", "auto")  # auto-detect by default
LOKI_BASE_URL: str = _get_env_str("LOKI_BASE_URL", "http://localhost:3100")
LOKI_TENANT: str = _get_env_str("LOKI_TENANT", "")
LOKI_TOKEN: str = _get_env_str("LOKI_TOKEN", "")
LOGS_JSONL_DIR: str = _get_env_str("LOGS_JSONL_DIR", "./logs")

DEFAULT_LIMIT = max(1, _get_env_int("LOGS_DEFAULT_LIMIT", 500))
HTTP_TIMEOUT_MS = max(1000, _get_env_int("LOGS_TIMEOUT_MS", 10000))
MAX_BYTES = max(5000, _get_env_int("LOGS_MAX_BYTES", 200000))

# JSON-RPC helpers
def create_response(request_id: Any, result: Any) -> Dict[str, Any]:
    return {"jsonrpc": "2.0", "id": request_id, "result": result}

def create_error(request_id: Any, code: int, message: str, data: Any = None) -> Dict[str, Any]:
    error = {"jsonrpc": "2.0", "id": request_id, "error": {"code": code, "message": message}}
    if data is not None:
        error["error"]["data"] = data
    return error

# Time parsing: Accept RFC3339 (with optional Z) or epoch seconds/millis/micros/nanos
def _to_ns(ts: Any) -> int:
    if ts is None:
        raise ValueError("timestamp is required")
    # Already int-like
    if isinstance(ts, (int, float)) or (isinstance(ts, str) and ts.isdigit()):
        n = int(ts)
        # Heuristics to convert to ns
        # If seconds: < 1e12; millis: < 1e15; micros: < 1e18
        if n < 10**12:      # seconds
            return n * (10**9)
        if n < 10**15:      # millis
            return n * (10**6)
        if n < 10**18:      # micros
            return n * (10**3)
        return n            # assume already ns
    # RFC3339-ish
    s = str(ts).strip()
    # Normalize Zulu
    if s.endswith("Z"):
        s = s[:-1] + "+00:00"
    try:
        dt = datetime.fromisoformat(s)
    except Exception as e:
        raise ValueError(f"Invalid timestamp format: {ts}") from e
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    epoch = datetime(1970, 1, 1, tzinfo=timezone.utc)
    ns = int((dt - epoch).total_seconds() * (10**9))
    return ns

def _now_ns() -> int:
    epoch = datetime(1970, 1, 1, tzinfo=timezone.utc)
    ns = int((datetime.now(timezone.utc) - epoch).total_seconds() * (10**9))
    return ns

def _since_to_start_ns(since: Optional[Any]) -> int:
    if since is None:
        # default last 5 minutes
        return _now_ns() - 5 * 60 * (10**9)
    # If numeric, treat as seconds back
    if isinstance(since, (int, float)) or (isinstance(since, str) and since.isdigit()):
        seconds = int(since)
        return _now_ns() - seconds * (10**9)
    # Else RFC3339 absolute start
    return _to_ns(since)

def _build_headers() -> Dict[str, str]:
    headers = {"Accept": "application/json"}
    if LOKI_TENANT:
        headers["X-Scope-OrgID"] = LOKI_TENANT
    if LOKI_TOKEN:
        headers["Authorization"] = f"Bearer {LOKI_TOKEN}"
    return headers

def _build_logql(query: str, labels: Optional[Dict[str, str]]) -> str:
    q = (query or "").strip()
    if labels:
        matcher = "{" + ",".join([f'{k}="{v}"' for k, v in labels.items()]) + "}"
        # If query already starts with a selector, don't try to merge deeply; just prefix our selector if not present
        if q.startswith("{"):
            # Leave as-is to avoid breaking valid LogQL. Advanced merging can be future work.
            return q
        if q and not q.startswith("|"):
            # Treat q as a pipeline/regex/line filter
            return f"{matcher} {q}"
        else:
            # Only pipeline ops provided (e.g., |~ ...), attach to matcher
            return f"{matcher} {q}"
    return q or "{job=~\".*\"}"

async def _loki_query_range(
    client: httpx.AsyncClient,
    query: str,
    start_ns: int,
    end_ns: int,
    limit: int,
    direction: str,
) -> Dict[str, Any]:
    params = {
        "query": query,
        "start": str(start_ns),
        "end": str(end_ns),
        "limit": str(max(1, min(limit, 5000))),  # hard cap 5k per request
        "direction": direction if direction in ("forward", "backward") else "backward",
    }
    url = f"{LOKI_BASE_URL.rstrip('/')}/loki/api/v1/query_range"
    r = await client.get(url, params=params, headers=_build_headers())
    r.raise_for_status()
    return r.json()

def _flatten_streams(res_json: Dict[str, Any]) -> Tuple[List[Tuple[str, Dict[str, str], str]], int, int]:
    """
    Returns list of (ts_iso, labels, line), streams_count, total_entries
    """
    out: List[Tuple[str, Dict[str, str], str]] = []
    data = res_json.get("data", {})
    streams = data.get("result", [])
    total = 0
    for stream in streams:
        labels = stream.get("stream", {}) or {}
        for ts_ns, line in stream.get("values", []):
            total += 1
            # ts_ns is string ns
            try:
                ns_int = int(ts_ns)
                ts_iso = datetime.fromtimestamp(ns_int / 1e9, tz=timezone.utc).isoformat()
            except Exception:
                ts_iso = str(ts_ns)
            out.append((ts_iso, labels, line))
    return out, len(streams), total

def _summarize_lines(
    entries: List[Tuple[str, Dict[str, str], str]],
    max_bytes: int,
) -> str:
    lines: List[str] = []
    size = 0
    for ts_iso, labels, line in entries:
        svc = labels.get("service_name") or labels.get("service") or labels.get("app") or "-"
        lvl = labels.get("level") or labels.get("severity") or "-"
        # Basic excerpt of line
        snippet = line.strip()
        # Build output line
        out_line = f"{ts_iso} [{svc}] [{lvl}] {snippet}"
        enc = out_line.encode("utf-8", errors="replace")
        if size + len(enc) + 1 > max_bytes:
            lines.append("[truncated]")
            break
        lines.append(out_line)
        size += len(enc) + 1
    return "\n".join(lines)

async def handle_initialize(_params: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "protocolVersion": "0.1.0",
        "capabilities": {"tools": {}}
    }

